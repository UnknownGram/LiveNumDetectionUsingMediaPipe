{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5173cef2-1724-4e39-8491-72db1645d59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (0.10.14)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: torch in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.4.30)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (1.24.1)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (4.10.0.84)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.4.7)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from torch) (1.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from jax->mediapipe) (1.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: rich in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\21ste\\anaconda3\\envs\\computervision\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python tensorflow torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca647e5d-c8ab-4521-87aa-d82d16176ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)  # Detecting only one hand\n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94ccb372-5158-4943-ba4f-1231a52df70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21ste\\anaconda3\\envs\\ComputerVision\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the image and find hands\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    # Draw hand landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow(\"Hand Landmarks\", img)\n",
    "    \n",
    "    # Break the loop on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f4277d-7de0-4d0a-b57b-b4af64a5c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[landmark {\n",
      "  x: 0.172768116\n",
      "  y: 0.970387816\n",
      "  z: 7.65904701e-007\n",
      "}\n",
      "landmark {\n",
      "  x: 0.269867331\n",
      "  y: 0.937782347\n",
      "  z: -0.0329808667\n",
      "}\n",
      "landmark {\n",
      "  x: 0.36486268\n",
      "  y: 0.857812285\n",
      "  z: -0.0415790305\n",
      "}\n",
      "landmark {\n",
      "  x: 0.441080451\n",
      "  y: 0.802101135\n",
      "  z: -0.0497339889\n",
      "}\n",
      "landmark {\n",
      "  x: 0.503159225\n",
      "  y: 0.7745381\n",
      "  z: -0.0562442206\n",
      "}\n",
      "landmark {\n",
      "  x: 0.333742261\n",
      "  y: 0.671423793\n",
      "  z: 0.00840052497\n",
      "}\n",
      "landmark {\n",
      "  x: 0.391335547\n",
      "  y: 0.57624054\n",
      "  z: 0.000256689498\n",
      "}\n",
      "landmark {\n",
      "  x: 0.422877252\n",
      "  y: 0.516356051\n",
      "  z: -0.0154764093\n",
      "}\n",
      "landmark {\n",
      "  x: 0.448996425\n",
      "  y: 0.463596225\n",
      "  z: -0.0288301148\n",
      "}\n",
      "landmark {\n",
      "  x: 0.281427681\n",
      "  y: 0.638478816\n",
      "  z: 0.00676353462\n",
      "}\n",
      "landmark {\n",
      "  x: 0.32194671\n",
      "  y: 0.513248\n",
      "  z: 0.00471798\n",
      "}\n",
      "landmark {\n",
      "  x: 0.348638117\n",
      "  y: 0.43202877\n",
      "  z: -0.00964828\n",
      "}\n",
      "landmark {\n",
      "  x: 0.37085855\n",
      "  y: 0.364884824\n",
      "  z: -0.0227040127\n",
      "}\n",
      "landmark {\n",
      "  x: 0.224193767\n",
      "  y: 0.633632123\n",
      "  z: -0.00219762512\n",
      "}\n",
      "landmark {\n",
      "  x: 0.256851405\n",
      "  y: 0.517437935\n",
      "  z: -0.0104285292\n",
      "}\n",
      "landmark {\n",
      "  x: 0.282126576\n",
      "  y: 0.446977586\n",
      "  z: -0.0263621043\n",
      "}\n",
      "landmark {\n",
      "  x: 0.304844141\n",
      "  y: 0.385703921\n",
      "  z: -0.0395233445\n",
      "}\n",
      "landmark {\n",
      "  x: 0.16008006\n",
      "  y: 0.649128199\n",
      "  z: -0.0158313718\n",
      "}\n",
      "landmark {\n",
      "  x: 0.156726852\n",
      "  y: 0.553309858\n",
      "  z: -0.0286566671\n",
      "}\n",
      "landmark {\n",
      "  x: 0.156110436\n",
      "  y: 0.489263088\n",
      "  z: -0.0366247371\n",
      "}\n",
      "landmark {\n",
      "  x: 0.160865724\n",
      "  y: 0.426461399\n",
      "  z: -0.0425438546\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if results.multi_hand_landmarks is not None:\n",
    "    len(results.multi_hand_landmarks[0].landmark)\n",
    "\n",
    "print(results.multi_hand_landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81a3c2a8-2f09-4668-92a0-e5846f24c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand = []\n",
    "for res in results.multi_hand_landmarks[0].landmark:\n",
    "    test = np.array([res.x, res.y, res.z])\n",
    "    hand.append(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706c163f-0c7a-43dd-bd91-5b8a2944b9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.10653356e-01,  8.40650797e-01, -8.40775485e-08]),\n",
       " array([ 0.20370644,  0.90473753, -0.01758477]),\n",
       " array([ 0.29257888,  0.95954144, -0.04289951]),\n",
       " array([ 0.35290524,  0.96841967, -0.06817947]),\n",
       " array([ 0.3727107 ,  0.9155637 , -0.09315211]),\n",
       " array([ 0.32863402,  0.86786121, -0.0621929 ]),\n",
       " array([ 0.32942146,  0.99669838, -0.11100143]),\n",
       " array([ 0.29948169,  1.02702463, -0.1414241 ]),\n",
       " array([ 0.28835338,  1.00905561, -0.16078572]),\n",
       " array([ 0.28022093,  0.84379929, -0.07724471]),\n",
       " array([ 0.26927474,  1.00618064, -0.11555979]),\n",
       " array([ 0.2457993 ,  1.01944971, -0.12600937]),\n",
       " array([ 0.24227858,  0.98674631, -0.13737567]),\n",
       " array([ 0.22025311,  0.83392233, -0.09363868]),\n",
       " array([ 0.21779531,  0.975546  , -0.12973042]),\n",
       " array([ 0.19887239,  0.99522042, -0.12114286]),\n",
       " array([ 0.19034666,  0.97910684, -0.11610265]),\n",
       " array([ 0.15931553,  0.83199322, -0.11164748]),\n",
       " array([ 0.16810706,  0.94331467, -0.13925406]),\n",
       " array([ 0.15612237,  0.98078161, -0.13185044]),\n",
       " array([ 0.14453167,  0.98608106, -0.12534849])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0a293c-b554-4a80-bd22-bf50b736d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    hand = np.array([[res.x, res.y, res.z] for res in results.multi_hand_landmarks[0].landmark]).flatten() if results.multi_hand_landmarks is not None else np.zeros(21 * 3)\n",
    "    return hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "92a1ee8f-8bbb-4271-849d-2992bc6463e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.72707832e-01,  7.82722890e-01, -1.50313284e-07,  6.25784099e-01,\n",
       "        7.46695697e-01, -1.75854843e-02,  6.57452822e-01,  6.56242549e-01,\n",
       "       -2.26993002e-02,  6.46410704e-01,  5.76803267e-01, -2.81400587e-02,\n",
       "        6.07787967e-01,  5.37198544e-01, -3.07809506e-02,  6.51542723e-01,\n",
       "        5.96962631e-01,  6.53833430e-03,  6.69347048e-01,  5.38327754e-01,\n",
       "       -1.10745430e-02,  6.65272474e-01,  5.66038489e-01, -2.79365834e-02,\n",
       "        6.55912220e-01,  6.00370944e-01, -3.74448411e-02,  6.19822383e-01,\n",
       "        5.81488073e-01,  5.23557281e-03,  6.31226063e-01,  5.04891753e-01,\n",
       "       -9.77924466e-03,  6.34571373e-01,  5.40347338e-01, -2.10995600e-02,\n",
       "        6.29491627e-01,  5.78044951e-01, -2.54081301e-02,  5.86334288e-01,\n",
       "        5.80359638e-01, -1.29718683e-04,  5.93971193e-01,  5.01294672e-01,\n",
       "       -2.02827398e-02,  6.04824841e-01,  5.45495212e-01, -2.26862691e-02,\n",
       "        6.00799322e-01,  5.86882055e-01, -1.96878370e-02,  5.50400257e-01,\n",
       "        5.89041650e-01, -7.00946385e-03,  5.71707785e-01,  5.36289513e-01,\n",
       "       -2.50317901e-02,  5.83415210e-01,  5.77493250e-01, -2.26863138e-02,\n",
       "        5.78943670e-01,  6.19091153e-01, -1.72714330e-02])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints = extract_keypoints(results)\n",
    "keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0f4057-f48b-4965-a434-4555a9ef69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Num_Data') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['1', '2', '3'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 3\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d073bfbf-b3f2-4de3-a339-cb7fd05d10b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\1\\\\0'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\1\\\\1'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\1\\\\2'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\2\\\\0'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\2\\\\1'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\2\\\\2'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\3\\\\0'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\3\\\\1'\n",
      "[WinError 183] Cannot create a file when that file already exists: 'Num_Data\\\\3\\\\2'\n"
     ]
    }
   ],
   "source": [
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f1ef57f-c9b7-48cb-9214-1a19bc5b25b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21ste\\anaconda3\\envs\\ComputerVision\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "# Start capturing video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through video length aka sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Read feed\n",
    "                ret, image = cap.read()\n",
    "\n",
    "                # Convert the BGR image to RGB\n",
    "                img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "                # Process the image and find hands\n",
    "                results = hands.process(img_rgb)\n",
    "\n",
    "                # Draw hand landmarks\n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                \n",
    "                # NEW Apply wait logic\n",
    "                if frame_num == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else: \n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    # Show to screen\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                \n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1cc3a7-78d5-4c83-8487-93e9bab54b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3333f882-e4ed-40e4-a03e-93fce1e6d9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39ce9eb-c16f-4986-831e-8fdf32f0cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab5f9931-b0a8-4315-8a26-8adf12bfdf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 0, '2': 1, '3': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba77eabd-c097-4598-9275-b3673a5113d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.59274721e-01,  7.70590544e-01, -3.41027260e-07,  3.14576119e-01,\n",
       "        7.45426059e-01, -2.00603362e-02,  3.67631406e-01,  6.64771438e-01,\n",
       "       -3.02429739e-02,  3.63141805e-01,  5.87185323e-01, -4.03786749e-02,\n",
       "        3.22023809e-01,  5.38140833e-01, -4.33878042e-02,  3.70929062e-01,\n",
       "        5.44777393e-01, -2.98266043e-03,  3.91915143e-01,  5.12260795e-01,\n",
       "       -3.70476730e-02,  3.76942247e-01,  5.78421831e-01, -5.49738742e-02,\n",
       "        3.63274604e-01,  6.01919234e-01, -6.40371367e-02,  3.26797038e-01,\n",
       "        5.23013532e-01, -9.81445052e-03,  3.43929201e-01,  4.94615167e-01,\n",
       "       -4.56106775e-02,  3.40266258e-01,  5.67422390e-01, -5.41689619e-02,\n",
       "        3.35127652e-01,  5.88014960e-01, -5.11386991e-02,  2.85688251e-01,\n",
       "        5.26484251e-01, -2.28794869e-02,  3.03584933e-01,  5.49114108e-01,\n",
       "       -5.90543114e-02,  3.08515340e-01,  6.21457040e-01, -5.81825934e-02,\n",
       "        3.05206627e-01,  6.34098291e-01, -4.95626666e-02,  2.50952184e-01,\n",
       "        5.48449814e-01, -3.85616310e-02,  2.78650016e-01,  6.23332202e-01,\n",
       "       -6.41553104e-02,  2.83156663e-01,  6.79585338e-01, -6.85318932e-02,\n",
       "        2.79868335e-01,  6.79183543e-01, -6.60386458e-02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.load(os.path.join(DATA_PATH, '2', '1', \"{}.npy\".format(0)))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2c804c-e100-4992-8a80-68938db36556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = np.load(os.path.join(DATA_PATH, '1', '1', \"{}.npy\".format(1)))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc50d6e6-1eda-4ee7-8cae-55fdfbbd752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            #sequences.append(label_map[action])\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88fff9ea-6774-4ab0-963c-343592629dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c83ef19-c23e-4e5e-9be2-c64cb8de39be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 30, 63)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91d244e-9e48-43de-9b79-612b85d26cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66f544e8-5b23-494c-9e8d-1e04bbb8e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50847b3-af69-400c-b850-2fa63f761e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 30, 63)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a0a5793-f2b4-4610-a489-8a1f9646d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "086af725-01ab-46c7-a75f-da4e1a816255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a3b7098-0d90-43bb-9d5f-67b63a4d10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "X = X.to(torch.float32)\n",
    "y = torch.from_numpy(y)\n",
    "y = y.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92a01c80-0d27-46d0-a593-8f25d8f4c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3103b06b-daa4-4c38-b37b-5683040eb14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 63])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abdaf9ff-6b3c-4f57-9a28-d0dbbae27985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 30, 63])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "766359c9-74ff-4a02-9b11-8973f6fbc02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, hidden_size3, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n",
    "        self.lstm3 = nn.LSTM(hidden_size2, hidden_size3, batch_first=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size3, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Take only the output of the last time step\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Assuming the input shape is (30, 1662) and actions.shape[0] is the number of classes\n",
    "input_size = 63\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 128\n",
    "hidden_size3 = 64\n",
    "output_size = 3  # Replace this with the actual number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50788878-539a-41a6-9864-dd4b54a96685",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "       \n",
    "    average_epoch_loss = epoch_loss / len(dataloader)\n",
    "    train_losses.append(average_epoch_loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Plot the training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'lstm_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb5372b1-b6ca-4235-98ed-44610cab747d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(input_size, hidden_size1, hidden_size2, hidden_size3, output_size)\n",
    "model.load_state_dict(torch.load('lstm_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc45cb01-9da6-49ee-918d-0e9949da33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(X_train[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cd5e148-7b34-4839-b658-87e41e649611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0012, 0.9977, 0.0011]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a24880d6-c81d-4e6d-9c6c-61cc327688fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[torch.argmax(res)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a088f7c7-062f-4025-9259-d248c549e5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[torch.argmax(y_test[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d9b44-d040-4b16-9d8b-635bc5d413d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "010d03f0-4886-42f8-992f-9fbfc4bd6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4540fc78-d90d-4b99-a937-ce99fef35ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4c4938e-7b7f-433b-b6cb-e15c164b298d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9877e-01, 1.2263e-03, 2.4858e-09],\n",
       "        [9.9878e-01, 1.2169e-03, 2.4326e-09],\n",
       "        [9.9864e-01, 1.3568e-03, 3.1771e-09],\n",
       "        [1.2058e-03, 9.9767e-01, 1.1277e-03],\n",
       "        [1.2492e-03, 9.9770e-01, 1.0537e-03],\n",
       "        [1.5947e-03, 9.9674e-01, 1.6687e-03],\n",
       "        [1.9519e-07, 2.3335e-03, 9.9767e-01],\n",
       "        [1.9498e-07, 2.3483e-03, 9.9765e-01],\n",
       "        [2.1884e-07, 2.4322e-03, 9.9757e-01]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f48bf1a5-3aba-425d-881e-95880bdb12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = torch.argmax(y, dim=1).tolist()\n",
    "yhat = torch.argmax(yhat, dim=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bd94b01-3b43-4c87-8f51-42a0be4c345f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 2, 2, 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51d1986b-012f-4265-af68-d75c0d8259de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 2, 2, 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50b92169-2fdb-4a91-b411-98cbed0c36d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[6, 0],\n",
       "        [0, 3]],\n",
       "\n",
       "       [[6, 0],\n",
       "        [0, 3]],\n",
       "\n",
       "       [[6, 0],\n",
       "        [0, 3]]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a70bab84-7195-4d84-9f96-19e2886bfe26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7856dc22-3f23-4417-b7f2-89c40ccf0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ec47002-9d91-42ed-878f-fef6dbd313a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8bde4c1-d942-47bc-a156-32347b790d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m18\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(prob_viz(res, actions, image, colors))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1800x1800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "plt.imshow(prob_viz(res, actions, image, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57f2dd8c-5a0d-4762-af79-1e170775932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21ste\\anaconda3\\envs\\ComputerVision\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "C:\\Users\\21ste\\AppData\\Local\\Temp\\ipykernel_13876\\2162121121.py:31: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  cSequence = torch.Tensor(sequence)\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "while cap.isOpened():\n",
    "\n",
    "    # Read feed\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    # Convert the BGR image to RGB\n",
    "    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and find hands\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    # Draw hand landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    # 2. Prediction logic\n",
    "    keypoints = extract_keypoints(results)\n",
    "    sequence.append(keypoints)\n",
    "    sequence = sequence[-30:]\n",
    "    \n",
    "    if len(sequence) == 30:\n",
    "        cSequence = torch.Tensor(sequence)\n",
    "        res = model(cSequence.unsqueeze(0))[0]\n",
    "        #print(res)\n",
    "        #print(actions[np.argmax(res)])\n",
    "        predictions.append(torch.argmax(res))\n",
    "        \n",
    "        \n",
    "    #3. Viz logic\n",
    "        if np.unique(predictions[-10:])[0]==torch.argmax(res): \n",
    "            if res[torch.argmax(res)] > threshold: \n",
    "                \n",
    "                if len(sentence) > 0: \n",
    "                    if actions[torch.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[torch.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[torch.argmax(res)])\n",
    "\n",
    "        if len(sentence) > 5: \n",
    "            sentence = sentence[-5:]\n",
    "\n",
    "        # Viz probabilities\n",
    "        image = prob_viz(res, actions, image, colors)\n",
    "        \n",
    "    cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "    cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Show to screen\n",
    "    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "    # Break gracefully\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "66b863a3-1dc9-4b53-b6b4-e564263cb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117daec0-a588-4bcd-8694-2a37e9efbfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
